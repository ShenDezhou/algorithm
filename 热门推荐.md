#数据制作

检索数据流程总结：

所有的数据最终文件都是14个字段，之间以\t分割。分别是：
内网url(没有就用外网url填充)
外网url(实际访问这条数据的url)
title 文章标题
nickname 发文账号的中文名称
account_id 发文账号id
page_time 文章时间，时间戳类型
content_text 文章内容
imgs 图片链接 多个链接之间以一个英文空格分割
video_sig 是否是视频，视频为1非视频补0或-1 图集是2
video_time 视频时长
hot_rate 文章热度
read_num  文章阅读数
praise_num 文章点赞数
spread_rate 文章传播率

注意，有的数据来源，14个字段并不是去不存在的，这个时候，没有的字段补0。

1：微信数据流程代码：
代码入口：
具体微信中的一些黑名单，数据指标的文件参考 “微信数据格式说明.doc”
注意：微信知乎流程请咨询；微信知乎策略请咨询；

2：新闻数据流程（新闻数据流程分两部分，一部分是实时的，是每一个小时从那边rsync一次新闻数据，另一部分是视讯网的数据）
2.1：实时的数据部分：通过rsync形式同步数据，然后解析之后推送到kafka中
    代码入口：
    136的机器上的rsync服务是我自己配的，具体如果需要修改的话请参考
    ps -aux | grep rsync 查看/usr/local/rsync/etc/rsyncd.conf 下的配置文件
    rsync --daemon --config=/usr/local/rsync/etc/rsyncd.conf
2.2： 非实时的数据，拿取的是视讯网的数据
    代码入口：
3：知乎数据（数据是通过kafka接口推送过来的）
      具体的知乎数据格式请参考：“知乎推送数据格式说明文档正式版 v1.21”
4：自己爬取的一些新闻数据 （这部分数据在218这台机器）
    4.1：cnbeta数据：代码入口：
        每小时爬一次cnbate网站，每次爬20页的内容，最终解析结果推送到kafka消息队列里面
    4.2：企鹅号数据+搜狗号数据
        数据来源是合作方推送到kafka里面，具体的文档参考“企鹅号数据分发规则.pdf”
            代码入口：
            后续如果出现问题，请咨询
    4.3：抓取微博的数据（一小时抓取一次，抓完放kafka中）
                代码入口：
                conf/ 目录下是配置抓取哪些栏目的配置
    4.4：一点咨询的数据
                数据来源：合作方以接口推送过来的数据 具体文档请参考“一点资讯接口规范.pdf”  接口方面请参考2017年7月21号乃乔发的邮件和2017年7月28号果果发的邮件：
                代码入口：
    4.5:图集数据
        数据来源：xml格式 一般隔两天会有一个大的更新，隔一天是一个微量更新 
        做数据流程代码所在机器：目前代码已经完成，只需要加入到crontab -e中就行，另外添加一下监控
        对接负责人：
    4.6:视频数据
        数据来源：是一个接口，直接请求会返回一批文件，文件的内容是一个规格的xml格式组成的数据。因为视频的数据每天都是全量更新的，所以我们需要根据时间来提取出今日的增量数据再入库。
    数据流程代码：
    字段描述请参考txt文档：视频接口文档说明.txt
    对接负责人：
    4.7 今日头条的数据
         数据来源：通过扫今日头条的url，然后再去网页库中拿取今日头条的数据，怎样扫库请参考扫库文档 扫网页库 
         代码入口：
        注意：在scan_fun.sh里面有两句代码是用来取头条文章内容的tag的，跟做数据流程没关系，如果哪天不需要了可以把这两行注释掉，具体说明在scan_fun.sh里面有代码注释。
绿皮统计文档：
1：统计绿皮的代码地址（奥特曼平台）：
    PV日志和二级页点击日志：无线网页uigs日志
    VR页点击日志：无线搜索gosou nginx日志
2：日志锚点详情信息（具体有疑问请咨询）：
3：绿皮统计需求文档："热门推荐-绿皮统计-new.docx"  具体问题：请咨询
4：查看绿皮统计的地方：
10237 流量统计 目前暂时不需要，已经停止调度
10001 二级页统计-各区域点击统计 统计二级页相关的内容
10157 搜索结果页-整体统计【新旧策略】：热门推荐整体统计
10207 搜索结果页-整体统计-【检索版本】：检索版本统计
10209 搜索结果页-整体统计-【协同版本】：协同版本
10257 搜索结果页-整体统计【检索加协同】：检索加协同
10185 搜索结果页-视觉区内【整体统计】：视觉区内整体统计
10213 搜索结果页-视觉区内-整体统计【搜索策略】：视觉区内检索策略
10215 搜索结果页-视觉区内-整体统计【协同版本】：视觉区内协同版本
10229 搜索结果页-视觉区内-【检索加协同】：视觉区内检索加协同
奥特曼平台代码如果有问题：请在 奥特曼服务讨论组 咨询@

3:邮件监控
    代码入口：
    因为这个代码略微有点乱，这里写一下整体的代码流程，我再代码里面    也有注释
    整体思路：
    邮件统计的是前两天的数据产出和入库情况，例如今天是2018年4月11号 ，统计的是2018年4月09号的数据的产出和入库情况。
    1：watch_dog/count_search_data.sh  计算总共产生了多少的数据
     记录数据产出分两类：
        1：一类是实时入库的，需要通过/search/odin/kong/elasticsearch/log/fast_log/* 下面的文件来统计，该文件记录了实时入库的数据量。
        2：另一类是一天一入库的，这部分数据存在/search/odin/kong/elasticsearch/tmp/kafka_daily/ 目录下，需要通过 wc -l 来计算数据的产出量。
    2：计算了入库多少的数据，最终输出数据是 网站site 生产量，消费量
        python watch_dog/combin_data.py 参数1，参数2， 参数 3
        接受三个参数 参数1：入库的数据日志 参数2：上一步计算计算出来的数据（产出有多少数据） 参数3：最终输出的文件
   3：把第二步计算出来的数据做成html的格式
   4：发送邮件

4：定时删除217上的数据脚本
因为217上的硬盘大小比较小，一旦硬盘满了，会导致做数据的流程失败，所以有一个定时删除数据的脚本

#PV上报
VR
编辑
pv上报

1. PV统计是否带入口
type:  hotRcmd
stype: 由以下几个字段构成，用“-”符号链接
more nomore  是否有二级页入口
搜索策略
数据来源类型
所属机房
vr展现位置（索引）
vr实际距离页面顶部的距离（像素数）
全部五条数据的url（以竖线分割）

eg: http://pb.sogou.com/pv.gif?uigs_productid=wapapp&uigs_t=1504579137471&uigs_uid=Vt1BEj-sh4zT9B82&query=%25D3%25E3%25CF%25E3%25C8%25E2%25CB%25BF&uuid=fc3ce20f-fce7-44c8-a617-6468131017ba&iploc=CN1100&userarea=%E5%8C%97%E4%BA%AC%E5%B8%82&province=%E5%8C%97%E4%BA%AC%E5%B8%82&guid=null
&wuid=AAGmnYUGGwAAAAqUHDQ44AUA1wA%3D&pid=sogouwap&porb=p&reSearch=0&ladn=0&ladbrand=0&scrnwi=412&scrnhi=732&
pixelwi=1442&pixelhi=2562&abtest=2&sfrom=result_up&lscache=0&speedtype=chunkedgrayuser&wuidHashCode=0&dp=1&type=hotRcmd&stype=noMore-n-s-djt-9-562-http%3A%2F%2Fsa.sogou.com%2Fsgsearch%2Fsgs_tc_news.php%3Ftencentdocid%3D20180523A0MQQR00%26req%3DmhsS4VrhG4nV_bOKleKPMDCsQjuYh3idLwNxFriDQGo%3D|http%3A%2F%2Fm.v.sogou.com%2Fvideoclips%2Fpfxxk6dvmfxf6nzsgaydimrz.html|http%3A%2F%2Fm.v.sogou.com%2Fvideoclips%2Fpfxxk6dvmfxf6nzrgqzdmmbt.html|http%3A%2F%2Fsa.sogou.com%2Fsgsearch%2Fsgs_tc_news.php%3Ftencentdocid%3D20180522A0SQDP00%26req%3DmhsS4VrhG4nV_bOKleKPMEoU0GVkryHWo1Edy2QPYSY%3D|http%3A%2F%2Fm.mangguozhu.net%2Fcoop%2Fkkpmd21o5.html&pos=&&_t=1504579137471

 

2. 滑动到展现区统计 (与第四个基本类似)

type : hotRcmd
stype : 由以下几个字段构成，用“-”符号链接
exposure
more nomore  是否有二级页入口
搜索策略
数据来源类型
所属机房
vr展现位置（索引）
vr实际距离页面顶部的距离（像素数）
点击上报

1. 标题
hotRcmd-title-{搜索策略}-{数据来源类型} -{所属机房}-{vr实际距离页面顶部的距离}

2. 各搜索结果
hotRcmd-{索引}-{host}-{展现类型}-{搜索策略}-{数据来源类型}-{所属机房}-{vr实际距离页面顶部的距离}
eg: hotRcmd-1-mp.weixin.qq.com-text-n-s-djt-562

3. 更多内容按钮
hotRcmd–more-{搜索策略}-{数据来源类型}-{所属机房}-{vr实际距离页面顶部的距离}

 

二级页 
环境数据
var uigs_para = {"uigs_productid":"wapapp","type":"hotRcmd",stype:“index”};
pv上报
eg：
曝光上报
1. 每次翻页上报：  dataIndex_x （x为第几页，从1开始）
点击上报
1.  后退按钮
2.  回到顶部按钮
3.  各搜索结果
普通：hotRcmd-{索引}-{host}-{展现类型}-{搜索策略}-{数据来源类型}-{所属机房}-{vr实际距离页面顶部的距离}
微博百科：hotRcmd-{索引}-{host}-{展现类型}-{二级索引}-{搜索策略}-{数据来源类型}-{所属机房}-{vr实际距离页面顶部的距离}
备注：微博百科根据展现类型（聚合或单条）分为list和非list，不管是否是list，都有二级索引（非list二级索引为0），数据来源均为协同数据
附录
展现类型：
多图：multi_pic
单图：single_pic
无图：text
视频（视频不再区分是否有图）：video
微博 :  weibo weiboList
百科 :  baike baikeList
搜索策略：
新策略 : n   老策略: o
数据来源类型
协同数据： x     搜索数据：s  两者都有: xs
所属机房

#后端服务
纯检索数据制作：
代码位置：
每日一次crontab任务：
每日一次任务启动程序：
实时入库启动程序：
协同过滤数据制作：
代码位置：
svn地址：
crontab任务：
启动程序：
协同数据入库代码：
小说信息制作入库：
代码位置：
线上机位置：
线上机分为5个机房，分别是，每个机房分别有5台机器ip分别是：
所有机房线上模块部署位置完全相同，各个模块均在路径下；
每个机房均配置一台redis数据库，对应关系为：
redis中db0存储协同query-urlid的数据，db1存储urlid-urlinfo的数据；
db10存储小说数据；
此外还有一台线下测试用redis库，其余信息与线上库一致；
线下模块部署：
daemon模块：
部署位置：
svn地址：
此路径下有两个启动脚本，start.sh启动端口号为8301,用于为线下测试环境提供一个稳定的服务，start_demo.sh启动端口号为8302,用于开发人员自行调试；
daemon服务请求参数解释：
请求一个demo服务的url格式如下;
http://:8302/?query=刘德华&size=10&reqtype=demo&idxorcde=1000&searchRuleType=n#
query：查询词
size：获得结果的数量
reqtype：请求类型，vr是用于为VR页提供服务，json用于为二级页提供服务，demo用于开发者调试会产生一个简易的各项信息全面的界面；
idxorcde：请求意图，这个参数线上服务时是从openhub传递过来的，线下自己调试需要自己填写；
本字段共有四个比特位，第一个比特标识是请求协同数据还是检索数据，1表示检索，0表示协同；第二个比特位标识是否有小说意图，1为有，0为没有；
第三个比特位标识是否有医疗意图，1为有，0为没有；第四个比特位标识是否有视频意图，1为有，0为没有；
searchRuleType：策略类型，n为新策略，o为旧策略，现在已经全部切换成新策略，写为n即可；
dochint模块：
部署位置：
svn地址：
启动脚本：start.sh
tupuhint模块：
部署位置：
启动脚本：hintstart_sh.sh
注：此模块是由实体推荐同学（）维护，这一位置下代码为当前最新代码，当实体推荐对代码进行更新时，需要下载新代码重新启动；
redis模块：
线下用redis库，其余信息与线上库一致；
ES模块：
此模块线下没有搭建，直接用的线上ES库；

