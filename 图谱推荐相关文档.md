#tupu recommender 点击率预估数据流程

1.线下实验代码位置：10.144.59.51:/search/odin/huangyue/wap_ctr_prediction/build_new.sh(整体训练+评测流程)   evaluate.sh(评测代码) 密码:noSafeNoWork@2016
2.线上数据制作位置:/search/odin/huangyue/wap_ctr_prediction/mk_entityFeature.sh 目前该流程未加入实体更新时间的更新，需要后续加入，百科实体数据更新这份数据的获取可以咨询黄跃，目前线上并未加入数据更新，后续需提单子建立更新流程
3.训练过程与训练数据思路，详细过程参加上述代码，我做实验的每一步都保留了：
    1.训练数据获取：正例：点击率较高的query-entity(用click.info.test数据)，负例：随机抽取没有点击的query-entity
    2.特征选取:由于我们的数据是实体推荐，所以特征选择从3个方面:query：hint返回的6个hq值，entity：ctr、百科pv、百科更新时间、类别特征（one-hot，73维）、是否有tag、实体图片得分、实体名称长度（one-hot），query-entity:qdb四路数据得分，其中统计数值特征做了取sigmoid的操作，使用平均值填充缺失数据，非统计特征进行了没有值得填充处理。
    3.train:使用Logistic Regression +L1正则，由于我们的模型特征较少，所以L1正则基本够用，而且前期没有进行特征选择，所以L1正则还有辅助选择特征的作用。
4.其他也可以参考这个ppt，ctr预估分享
query、query-entity相关特征获取流程
可使用sggp打压力工具获取query、query-entity特征

#热门新闻数据推荐结果质量过滤逻辑

目标：由于热门新闻很多都很长尾，导致tupuhint给的hintquery不相关，后续推荐结果也导致不相关，使用在searchhub能够拿到网页返回的结果，通过看推荐结果与网页返回结果的相关性可以判定推荐效果是否ok。

方法：
整体方法：先从vr获取到人工运营的新闻热词，然后判断这些热词hint给出的结果或者最终给出的推荐实体的结果是否ok，如果不ok则直接干掉。
1.新闻数据制作代码:
   这里面包括所有的数据处理流程，目前该流程只是线下处理流程，线上数据已经打包为docker流程，
   docker流程数据制作位置:
   运行位置:使用第一台机器build完成之后打包成docker镜像，然后再使用clone到93.143的机器上运行，运行脚本为:
   每15分钟产出一次数据，运维自动同步到线上tupuhint机器上。tupuhint在判断query是新闻词时会在结果中填充hintlevel=1,如果不是新闻词，则hintlevel=0

2.tupuhint处理方法：
    人工取了100条新闻数据，选取query长度，得分，词性等特征，训练决策树，进行过滤.
    训练代码:这个代码主要是数据处理，特征抽取，模型训练使用的是weka的j48模型，在windows上进行的。
    线上代码:

3.searchhub处理方法：
由于hint过滤并不能完全搞定，因此在searchhub也做了一版过滤，这里主要用推荐的实体的topic向量和网页结果的topic向量比较，当拿到推荐结果中hintlevel=1的情况下，走过滤逻辑，把topic过滤的阈值卡的更严。
代码: searchhub的

#TEXT数据制作流程

text数据概述：挖掘新增的高质量query(高pv)，获取query文本(搜索结果页前几条结果的title、summary)，query文本与实体文本倒排求交获取top实体，计算query与实体相关性

text数据是每周更新一次

一、query、query文本、实体文本制作流程
数据流程位置：
入口脚本：
代码流程：
1）计算挖掘新增的高质query，最后生成新增query文件：hq_add
2) 抓取新增query的搜索结果页面
3)解析query搜索结果页，获取搜索结果页文档的title、summary制作成query文本向量
4）倒排求交计获取query召回的相关实体，结果为doc.term.text.0
这个脚本的逻辑是将query的文本向量发送到的query与实体倒排求交的服务，返回query与候选实体相关度打分
服务位置：

二、计算query与entity文本相关度(版本2)
使用matchlstm深度模型计算query与entity的文本相关度
代码位置：
代码逻辑流程：
1）从同步query搜索结果页，doc.term.text.0数据
2）解析query网页结果，获取query文本数据，并分词
3）使用深度模型计算query与候选实体的文本相关性
4）将结果去重、排序、归一化后存储在hdfs上，路径为晨曦集群：

计算query与entity文本相关度(版本1)
使用dnn模型计算query与entity的文本相关度
代码位置：

#tupuhint 相关文档
tupuhint模块相对简单，代码信息参考这个文档
下面简单介绍下代码结构与逻辑：
1.代码位置:
2.本地开发环境:
3.代码结构:
   整体采用的tcp 连接框架，多线程单一流水处理，Server代码: 每次接收一次请求在函数进行处理,在get_hint中调用函数.
   hint模块自带缓存，是实现在代码中的缓存，通过WebHint.cpp的HintGen（）的构造函数控制，默认开启。
   HintGen是WebHint命名空间下的类，在Hint.cpp/WebHint.cpp文件中，采用单例模式，set/getHint等参数均在其中，供QueryServer.cpp/QueryTask.cpp调用。
4.代码逻辑:
    getHint:每次接收到一个用户query后，先判断query长度是否符合阈值，如果不符合则不处理；对于长度符合限制的query进行分词，判断是否命中缓存，命中缓存则直接返回缓存结果，不然则走计算hint的逻辑。
    计算hint query的过程大致就是利用分词的term对每个term求倒排链，然后对倒排中所有的doc进行rank（并不会求交），rank的公式为中的公式，然后针对不同的case又加了一些规则控制，另外，tupu_hint代码    目前在rank过程中加入了query紧密度信息，会计算query中任意两个相邻的term之间的紧密度，如果紧密度大于0.9则要求doc中必须同时包含这两个term，以此来保证实体一定能被放在一起。
    同时在rank过程中采用的是omp多线程查询，由于有一些term的倒排链会很长，所以在这里遍历倒排链的时候，对代码运行时间设置了超时控制，如果在一定时间内没有遍历完则直接返回当前最好的结果。
    紧密度模块和hint模块之间是http连接，紧密度模块在另一个文档中介绍。tuputightness相关文档。 hint深度学习dssm模型，在dssm相关文档。

5.线下数据制作:
    线下数据制作10.138.25.224的大的做数据流程里，每天定时做，也可参考图谱推荐 中的数据制作流程。

6.线下工具：

#dssm 用于 tupuhint 优化
1.训练数据构成，正例：共生点击，负例随机生成，pairwise，1:1 具体参见上述训练数据制作代码。
2.模型目前分单机单卡版，和单机多卡版本，单机单卡版本卷积是手工写的代码实现的，并不是用的tensorflow的卷积。单机多卡版本由于训练输入数据的读取问题，需要使用tensoflow版本的卷积，目前使用tensorflow版本的卷积训练的模型效果不如使用手工写的版本效果好，所以上次评测使用的是单机单卡版本的代码。
3.线上代码入口:

#tupu tightness 模块相关文档
tupu tightness模块主要是用来解决实体识别的问题，这里我们使用的是rank组的紧密度模型和代码，我们仅仅是在外面又包装了一层http服务，目前该模块的模型和代码并不进行更新，如果需要更新联系rank组同学负责该模型的同学更新。
之前测试测的极限qps为1700，
附上重大上线单：
接收请求为utf16编码的query，返回结果为json.
也可以通过http请求直接访问:
代码：
本地调试环境:
测试工具：
线上机器（由于线上机器资源不够，这个模块是部署在interest机群上的）：
线上数据为二进制数据，为rank组制作。
另外，目前该模块的trunk代码（并未上线）加入了分词结果信息，请求中带上reqtype参数即可，

#共击数据生成流程
方法：tfidf方法求相似度，把query当做文档，处理过的url当做单词即可，url处理就是下述第一步，求出相似的query对之后会用entity.name去过滤，保证query后面是entity
1、把url的最后一级去掉，如果原来只有2级直接舍去该数据，原来有三级的话不用去掉最后一级
代码位置
2、生成query-click-click得分（得分是该query点击的click数目除以该query总的点击数目，之后取log得到）
代码位置
输出 query+"\t"+url 
输出 query+"\t"+url+"\t"+query在这个url上的得分
3、生成query对，输出格式为query1"\t"query2"\t"query得分"\t"对应的click\001click的idf\001query1的click得分\001query2的click得分
代码位置
输出  url+"\t"+query+"\t"+query在这个url上的得分
输出   query1"\t"query2"\t"query在这个url上的得分"\t"对应的click\001click的idf\001query1的click得分\001query2的click得分
4、将2得到的数据相同的query对合并，输出格式为query1"\t"query2"\t"query总得分"\t"对应的click1\001click1的idf\001query1的click1得分\001query2的click1得分  对应的click2\001click2的idf\001query1的click2得分\001query2的click2得分。。。。。有多少个共击click都展示出来
代码位置
map对于一个输入会产生2个输出，分别是query1"\t"query2"\t"query得分"\t"对应的click\001click的idf\001query1的click得分\001query2的click得分     query2"\t"query1"\t"query得分"\t"对应的click\001click的idf\001query2的click得分\001query1的click得分
5、生成query候选列表  输出格式为query1,query2,query1和query2总得分，对应的click列表得分，query3，query1和query3的总得分，对应的click列表得分。。。。。。
代码位置
6、最终总的实现代码：10.134.64.101 /search/odin/wangqianqian/common_click/get_query_score_all.sh
7、根据得到的query-entity，以及他们的得分，可以卡一个阈值，高于这个阈值的我们认为是相似的，这就需要看数据了。
注意：在算query在某个url的得分的时候，需要乘以一个log（query.pv），这样做的原因是认为pv量大的时候我们算的得分是更加可信的。     

#新闻数据dockers
1、mysql数据取回30条   mysql_data.py   输出数据库标题和图片url
2、标题过滤（搜狗热搜榜-今日热点标题去除）   filter.py    输出16条数据  
3、图片url固化和裁剪   pic_solid.sh
总的数据流程代码
每30分钟更新一次，线上每15分钟同步一次数据，上线单
更新后的数据保存位置
代码部署

#entity-search平台搭建
注意：在得到数据的过程中，一个月的展现低于10的数据被过滤掉了（过滤代码在：里面，限制了show的数量必须大于10才会输出 ）

补充：无线端的点击展现合成代码逻辑： 由于展现和点击的uuid对不上，我们用query来作为合成依据，首先把相同的query的点击和展现放到一个list里面，然后对于这个query的实体，用该实体的linkid来标识，具体代码请见：

#solr用法
solr的目录在
开启solr：bin/solr start (-m 表示要开启的jvm内存  -p表示要开启的端口号  -e表示以某个example的配置开启solr    其他用法请加-help查讯）
solr的界面地址：'http://localhost:8983/solr/（界面可以直接查询、建立核）
当我们需要用solr来查询数据的时候，首先需要有一个核，核里面会存放数据所需要的配置信息，以及以后的数据索引
建立核的方法：curl 'http://localhost:8983/solr/admin/cores?action=CREATE&name=cat_en&instanceDir=/search/odin/solr_platform/solr/example/cat_en/solr/cat_en&config=/search
/odin/solr_platform/solr/example/cat_en/solr/cat_en/conf/solrconfig.xml&schema=/search/odin/solr_platform/solr/example/cat_en/solr/cat_en/conf/managed-schem
a&dataDir=/search/odin/solr_platform/solr/example/cat_en/solr/cat_en/data'
上述命令建立一个名为cat_en的核，我们需要指定核的目录instancedir还有配置文件的目录config、schema（其中schema里面包含了这个核可以使用的字段，当我们需要增加字段或者删除字段的时候需要在这个文件里修改，并且要重新上传这个核才会生效。字段就是用来标识我们数据的每一列。）还要指定存放数据的目录datadir。

图谱推荐相关数据查询平台建立(进程启动代码):
初始化代码（仅在机器迁移时使用）:

上传数据到cat_en核并且建立索引：curl 'http://localhost:8983/solr/cat_en/update?commit=true' --data-binary @cat_en_data/test1 -H 'content-type:application/json'
（其中@后面是数据的位置 -H表示要上传的数据是json格式的）（需要注意的是curl每次传输的数据量是有限制的，大概20G以下都可以一次传输，所以有时候需要分次传输数据）

数据查询：curl"http://10.144.48.68:8983/solr/cat_en/select?q=entity%3A"好"&wt=json&indent=true"（q表示要查询的字段是entity，并且返回entity中包含“好”的数据，wt是返回数据的格式要求是json格式，indent表示json数据需要格式化）

核里面的索引的删除方法：curl 'http://localhost:8983/solr/admin/cores?action=UNLOAD&deleteIndex=true&core=cat_en'（如果不删除索引的话，这个索引会一直都存在，每条数据都有一个唯一的id来标识，如果新上传的数据的id和以前的索引冲突，那么它会覆盖原数据）

删除核的方法：curl 'http://localhost:8983/solr/admin/cores?action=UNLOAD&core=cat_en'

重新下载核的方法：curl 'http://localhost:8983/solr/admin/cores?action=RELOAD&core=cat_en'

停止solr ：bin/solr stop(-p 表示要停止某个端口号  -all表示将所有的solr进程停止）

