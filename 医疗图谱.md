下面分几个模块来介绍整个医疗图谱
一、数据抓取/解析/预处理
明医项目需要抓取好大夫/寻医问药/360jk等站点的数据，以及后续的更新流程，主要是现在109机器上做抓取和预处理工作然后将处理后的文件scp到43机器上去build图谱数据
（1）明医数据处理脚本
    这是明医项目的抓取解析脚本位置，脚本中包含了抓取，解析，归一化部分，最终处理成明医做数据的初始文件
    处理好大夫的医院基本信息的脚本
    处理好大夫的科室详情页的脚本
    处理好大夫医院的详情首页数据
（2）爬虫模块
    目录：
    这个是爬虫模块，这里抓网页主要是两种方式
    i.  dbnetget    扫网页库  参考bin/scan_html_xpage.sh  &  bin/decode_html_xpage.py
    ii. crawler.py  直接抓取
（3）页面解析模块
    目录：
    这个是针对Crawler步得到的页面进行解析，解析出你需要的数据和对应格式 常见的解析方式有如下几种：
    i.   正则表达式，参考好大夫医生详情页的解析脚本
    ii. xpath解析，个人认为用xpath解析网页比较方便，但lxml库内之功能比较晦涩难懂，因此基于lxml库制作一个小函数以供调用；
        代码路径：
        getPage函数：根据输入url返回网页的源码，此函数对重传，加装headers以及压缩数据格式的解压等均在内部做了相应的处理。
        str_xpath函数：入参con为Unicode格式HTML源码的字符串，path为所要获取的节点的xpath路径的字符串，函数返回值为所有在con里满足path路径的节点的Unicode格式字符串的list。
    iii. node的cheeiro，利用js的css选择器进行元素值的获取  参考 
    本项目对应在NodeParser目录下
    使用方法：node page_parser.js data_path（data_path格式是一行url,一行html,要求是utf8编码）
    如果有新的页面需要解析的话，需要针对页面写解析脚本，参考（好大夫详情页的解析脚本），
    然后在page_parser.js中加载，并在urlDispatch函数中编写对应的要解析的domain
（4）明医图片固化模块：
二、数据构建
离线数据流程主要代码位置以及相关功能：
离线数据制作流程：
1.    整个数据制作的所有数据源均在data/oridata 下，但这些数据源要有统一的格式，此过程在data/rawdata下执行，首先把数据存成[-1\t来源网址\t实体名\t属性名\t属性值\t-1]格式，然后通过bin/tuplenorm.py程序将数据做成标准格式传到目录data/oridata 下；
2.      源数据搞好后就可以开始数据制作了，即执行build.sh程序；
3.      build数据结束后要把VR所需的XML文件重新生成并往开放平台传送一份；（具体过程后面会说）
医生评论数据制作：
由于医生评论的数据量较大，医生评论的数据制作被抽出来单独来做了。
抓取代码路径：
抓取解析代码：
Build数据代码：
此数据虽然与主体build数据分开，入得也不是同一个数据表，但是有一个评论数统计的值需要加到总库当中，因此制作过程中会在data/oridata/下生成一个文件haodf_patient_comment_inf.norm，此文件中记录了这些统计数据，下次build数据的时候就能合入了。
前期数据制作代码位置以及相关功能：
1.该路径下为丁香园、好大夫、梅奥医院等站点的抓取和解析代码
2.该路径下有扫库和抽取页面内容的脚本。
（a）    get_page.sh输入：url，输出：xpage格式的pages。对应解析各个字段的工具是get_html_from_db.py
（b）    get_pagehtml.sh输入：url，输出：包含网页html的pages。对应解析html的工具是get_html_from_db.py
3.计算医院排行榜：是医院排行榜数据结果，为计算脚本（具体可以参见交接文档）
4.好大夫推送数据：
三、mingyi qo模块
        医院二级页搜索功能，涉及到一个query解析的模块，主要功能是suggestion和query解析，主要方法是利用所有子串构建倒排索引，结合词表来解析
所以需要先建立索引，然后利用索引文件和词表，解析query
（1）建立索引
    入口是：bin/Indexer.py
    现在使用的是：做索引
    需要根据 conf/index.ini 文件准备数据 
    将做好的索引数据文件拷贝到215机器上  即
    (也可以将43上做索引的步骤放在215机器上做)

（2）qo search server
    入口参考  run.sh

四、mingyi 医生搜索qo模块
      整个模块的功能就是先为医生、医院、地区、科室建立倒排索引，然后根据用户的输入query解析出query中含有那些信息，根据query给出提示词的suggestion。
      代码路径：
      conf/index.ini：整个系统的配置文件
      bin/EntityNode.py：数据节点类的定义文件
      bin/Indexer.py：建索引程序
      bin/Searcher.py：搜索query解析和搜索query suggestion代码
      bin/mingyidoctorQOServer.py：建立医生搜索服务的代码
      dic/：医院、科室、医生的原始数据文件
      output/：建索引生成的文件都在这里

五、VR数据制作
目录：上面的数据主要都是针对医院二级页的，如果需要在明医搜索中以VR的格式展现，则需要给各种VR生成xml文件，放到指定的ftp服务器的指定目录下，开放平台会定期抓取
注意：医院二级页的图谱数据有变化时，需要修改VR的xml数据，以保持搜索页的VR结果要与二级页的结果一致
（1）疾病对应科室的VR数据生成
二者区别：旧的VR数据中只含有科室的信息，但由于新的需求需要推荐医院，因此新的VR数据在原有医院的科室中选择第一个科室的数据里加入了医院本身的信息；
 （2）医院评论的VR数据生成
 （3）医院一些就医/交通攻略的VR数据生成
 （4）医生VR数据生成
这些数据生成后需要手动传到开放平台：，具体命令在程序最后一行有体现。

六、数据存储和编辑平台
（1）   存储DB机器
              三台机器数据库表结构都是一样的：
              area : 全国各省、市、区县的地区信息，供前端页面展示使用
              doctor_comment : 医生评论数据
              fake_drugs : 假药数据
              keshi：科室信息数据，供前端页面展示使用
              mingyi_doctor_norm : 存放拆分后的医生数据，现在处于闲置状态
              mingyi_hospital_norm : 存放拆分后的医院数据，现在处于闲置状态
              mingyi_info：医院和医生合并的数据，各个属性的所有来源的属性值都保存
              mingyi_norm：也是医院和医生合并的数据，从各个属性的所有来源的属性值中选取一个优先级最高的保存
              mingyi_manual下也有一个mingyi_norm，这个表是专门用来保存人工编辑平台的修改值，在build数据的最后一步要把人工平台的修改数据合入
（2）   明医编辑平台
            用于明医数据的查看/修改 （即对错误的数据进行人工编辑）
（3）   线上机器
（4）常规上线类型
        1. 数据上线
            参考运维例行上线单：
           即在产品修改了名医编辑平台的数据后，或者原始的名医数据有更新时，要想让数据在线上生效，需要提这个上线单
           上线之前最好在线下测试后确认没问题后再上，配本机然后在上查看
        2. qo server上线
           参考 运维例行上线单子：
          即当搜索/suggestion的逻辑有变化，或者疾病对应科室的关系有变化时，需要先做索引，然后更新索引数据到SVN 打tag上线
